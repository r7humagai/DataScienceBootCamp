{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNbu7CAREU4gTCTFWyqov6A"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Natural Language Processing**\n",
        "Part of computer science and artificial intelligence which deals with human languages.\n",
        "\n",
        "**Applications**\n",
        "1. Spell Checking\n",
        "2. Keyword Search\n",
        "3. Information Extracting\n",
        "4. Advertisement Matching\n",
        "5. Sentiment Analysis\n",
        "6. Speech Recognition\n",
        "7. Implementation of Chatbot\n",
        "8. Machine Translation\n",
        "\n",
        "**Components**\n",
        "1. Natural Language Understanding\n",
        "  - Mapping input to Useful Representations\n",
        "  - Analyzing different aspects of the language\n",
        "\n",
        "2. Natural Language Generation\n",
        "  - Text Planning - Retreving contents from the knowledge base.\n",
        "  - Sentence Planning - Choosing required words from meaningful phrases setting tone of the sentence.\n",
        "  - Text Realization - Mapping sentence plans into sentence structure\n",
        "\n",
        "**Ambiguities**\n",
        "  - Lexical Ambiguity - Presence of two or more more meaning within a single word.\n",
        "  - Syntactical Ambiguity - Presence of two or more meaning within a single sentence or a sequence of words.(Structured or Grammatical Ambiguity)\n",
        "  - Referential Ambiguity - When a word or phrase can be interpreted to refer to more than one item.\n",
        "\n",
        "**NLTK**\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "ThGQt4UP3r8R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "----\n",
        "#Tokenization"
      ],
      "metadata": {
        "id": "vmzAbymP80hn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import nltk\n",
        "import nltk.corpus"
      ],
      "metadata": {
        "id": "0eFJCFKh37_E"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zL-r9vmk7hIA",
        "outputId": "b7801d6d-08f9-4d41-b995-6b67b567375c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> q\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#print(os.listdir(nltk.data.find(\"corpora\")))"
      ],
      "metadata": {
        "id": "LjKzhH257BIV"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('brown')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aBLWGbXt8gst",
        "outputId": "3dfdb36d-8a35-489f-a790-4b6e3458adaa"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import brown\n",
        "brown.words()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "czLrgxwk8SzF",
        "outputId": "8882374a-9bc8-42e2-f86f-d59ddbac6a41"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('gutenberg')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r9A5F6lP8psz",
        "outputId": "7a0664d2-9320-4052-9b29-0f7a66ebba5f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.corpus.gutenberg.fileids()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tGOnxHZm7BKG",
        "outputId": "badde960-0e1f-42e7-e605-458859ee1f9b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['austen-emma.txt',\n",
              " 'austen-persuasion.txt',\n",
              " 'austen-sense.txt',\n",
              " 'bible-kjv.txt',\n",
              " 'blake-poems.txt',\n",
              " 'bryant-stories.txt',\n",
              " 'burgess-busterbrown.txt',\n",
              " 'carroll-alice.txt',\n",
              " 'chesterton-ball.txt',\n",
              " 'chesterton-brown.txt',\n",
              " 'chesterton-thursday.txt',\n",
              " 'edgeworth-parents.txt',\n",
              " 'melville-moby_dick.txt',\n",
              " 'milton-paradise.txt',\n",
              " 'shakespeare-caesar.txt',\n",
              " 'shakespeare-hamlet.txt',\n",
              " 'shakespeare-macbeth.txt',\n",
              " 'whitman-leaves.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code is using NLTK to load the words from the Gutenberg corpus that contains the text of Shakespeare's play 'Hamlet'. It essentially retrieves a list of words from the specified text file."
      ],
      "metadata": {
        "id": "vJBucX6s9mBY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hamlet = nltk.corpus.gutenberg.words('shakespeare-hamlet.txt')\n",
        "hamlet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8OylvaMv9RcC",
        "outputId": "3f03b55c-e1c6-41f4-9650-fdef3ecf220d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[', 'The', 'Tragedie', 'of', 'Hamlet', 'by', ...]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The variable 'hamlet' contains a list of words from the play 'Hamlet' allowing us to perform various text analysis tasks using NLTK on this particular text.\n",
        "For eg. we could analyze word frequencies, perform sentiment analysis, or any other NLP-related tasks on the text of 'Hamlet'."
      ],
      "metadata": {
        "id": "Q58ntPVe91Cz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for word in hamlet[500: 515]:\n",
        "  print(word, sep = ' ', end = '\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f-_hW6Is-ITA",
        "outputId": "88815c95-d0f7-4ff5-ad6a-d8d0e61a1373"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "thou\n",
            "that\n",
            "vsurp\n",
            "'\n",
            "st\n",
            "this\n",
            "time\n",
            "of\n",
            "night\n",
            ",\n",
            "Together\n",
            "with\n",
            "that\n",
            "Faire\n",
            "and\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "AI = \"\"\"According to the father of Artificial Intelligence, John McCarthy, it is “The science and engineering of making intelligent machines, especially intelligent computer programs”.\n",
        "Artificial Intelligence is a way of making a computer, a computer-controlled robot, or a software think intelligently, in the similar manner the intelligent humans think.\n",
        "AI is accomplished by studying how human brain thinks, and how humans learn, decide, and work while trying to solve a problem, and then using the outcomes of this study as a basis of developing intelligent software and systems.\n",
        "Philosophy of AI\n",
        "\n",
        "While exploiting the power of the computer systems, the curiosity of human, lead him to wonder, “Can a machine think and behave like humans do?” Thus, the development of AI started with the intention of creating similar\n",
        "intelligence in machines that we find and regard high in humans.\n",
        "\n",
        "Goals of AI\n",
        "• To Create Expert Systems − The systems which exhibit intelligent behavior, learn, demonstrate, explain, and advice its users.\n",
        "• To Implement Human Intelligence in Machines − Creating systems that understand, think, learn, and behave like humans.\n",
        "\n",
        "What Contributes to AI?\n",
        "Artificial intelligence is a science and technology based on disciplines such as Computer Science, Biology, Psychology, Linguistics, Mathematics, and Engineering. A major thrust of AI is in the development of computer functions associated with human intelligence, such as reasoning, learning, and problem solving.\n",
        "Out of the following areas, one or multiple areas can contribute to build an intelligent system.\"\"\""
      ],
      "metadata": {
        "id": "eHI1Hg3a-1me"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(AI)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k6dNSoSJ_NCQ",
        "outputId": "11d34133-09e5-470e-e0a7-761ce0541062"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**word_tokenize Function**\n",
        "\n",
        "Function from nltk, used for tokenizing a text into words."
      ],
      "metadata": {
        "id": "vV8tyGttBlMk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize   # Tokenization is the process of breaking down a text into smaller units, such as sentences or words."
      ],
      "metadata": {
        "id": "WMyrFpC2_TGQ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')    # a pre-trained unsupervised machine learning model for tokenizing sentences into individual words.\n",
        "# It works by using an unsupervised learning algorithm to learn the most likely sentence boundaries based on the distribution of words in a given language.\n",
        "# In NLTK, used for sentence segmentation -> break a text into sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37sUBKKX_jkJ",
        "outputId": "6a69808e-9de2-4b11-b065-ab2d1868fb23"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "AI_tokens = word_tokenize(AI)     #-> Saves as a list\n",
        "AI_tokens[5:20]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fRWo9gX3_TKa",
        "outputId": "839a62e1-f5bb-4054-f7d5-834b9f494afb"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Artificial',\n",
              " 'Intelligence',\n",
              " ',',\n",
              " 'John',\n",
              " 'McCarthy',\n",
              " ',',\n",
              " 'it',\n",
              " 'is',\n",
              " '“',\n",
              " 'The',\n",
              " 'science',\n",
              " 'and',\n",
              " 'engineering',\n",
              " 'of',\n",
              " 'making']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(AI_tokens))\n",
        "print(len(AI_tokens))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9VIOlTLkAXqc",
        "outputId": "8e0fbadb-c5f0-4b00-f7a9-8b249c10eb25"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n",
            "286\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Frequency Distribution Class(FreqDist)**\n",
        "\n",
        "Class in NLTK that helps for analysis of frequency of each element in a dataset. Or, analyze the frequency of words in a text."
      ],
      "metadata": {
        "id": "Z-HqBE5cA4hZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.probability import FreqDist\n",
        "fdist = FreqDist()"
      ],
      "metadata": {
        "id": "JLwtxejY_TM-"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word in AI_tokens:\n",
        "  fdist[word.lower()] += 1\n",
        "fdist"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lrzJeDG4B18U",
        "outputId": "2f15e5de-cff3-49d4-c0fc-8cdcd9b53296"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FreqDist({',': 30, 'of': 14, 'the': 13, 'and': 12, '.': 9, 'a': 9, 'to': 7, 'intelligence': 6, 'intelligent': 6, 'ai': 6, ...})"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the frequency of each word\n",
        "print(fdist['intelligence'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kluni0qrCcn0",
        "outputId": "7ccb35a4-77a4-48e4-b39a-4cb72f46bef2"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(fdist))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6X2F-yQ9Ci5s",
        "outputId": "9f3a7266-bae1-4d43-cf67-fbf11dff26e9"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "126\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Top ten tokens with highest frequency\n",
        "\n",
        "fdist_top10 = fdist.most_common(10)\n",
        "print(fdist_top10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vq3mqUoYIVb7",
        "outputId": "6a99be22-9928-4e24-9f62-49cb491dd89e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(',', 30), ('of', 14), ('the', 13), ('and', 12), ('.', 9), ('a', 9), ('to', 7), ('intelligence', 6), ('intelligent', 6), ('ai', 6)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**blankline_tokenize**\n",
        "- Tokenizing a text into chunks based on blank lines.\n",
        "- Designed to split a text into sections separated by one or more blank lines."
      ],
      "metadata": {
        "id": "uvWnAv4BIsNx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import blankline_tokenize\n",
        "AI_blank = blankline_tokenize(AI)\n",
        "len(AI_blank)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9DUOEE3gIrYn",
        "outputId": "98715855-9855-4560-8cad-2e7c5014da18"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "AI_blank[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "rGJlp84AIriX",
        "outputId": "2c7c36f3-4cda-4342-e615-6b1b5e432137"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'According to the father of Artificial Intelligence, John McCarthy, it is “The science and engineering of making intelligent machines, especially intelligent computer programs”.\\nArtificial Intelligence is a way of making a computer, a computer-controlled robot, or a software think intelligently, in the similar manner the intelligent humans think.\\nAI is accomplished by studying how human brain thinks, and how humans learn, decide, and work while trying to solve a problem, and then using the outcomes of this study as a basis of developing intelligent software and systems.\\nPhilosophy of AI'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tokenization\n",
        "- Bigrams - Tokens of two consecutive written words as Bigrams\n",
        "- Trigrams - Tokens of three consecutive written words as Trigram\n",
        "- Ngrams - Tokens of any number of consecutive written words known as Ngrams."
      ],
      "metadata": {
        "id": "EJHchrVdKPKx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bigrams** provide context by capturing relationships between adjacent words. They help understand how words are often used together. Use example: Predicting the next word in a sequence.\n",
        "\n",
        "**Trigrams** capture relationships between three words, offering more information about the structure and flow of the text. Use example: Part-of-Speech tagging and Named Entity Recognition."
      ],
      "metadata": {
        "id": "YjKLHFBdnO7G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.util import bigrams, trigrams, ngrams"
      ],
      "metadata": {
        "id": "fOz9SBe2KhTZ"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "string = \"The best and most beautiful things in the world cannot be seen or even touched, they must be felt with the heart\"\n",
        "quotes_tokens = nltk.word_tokenize(string)\n",
        "quotes_tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J5WyuWWwK1Ox",
        "outputId": "e8173b87-e4d9-413b-a221-49bbaa8c9b45"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The',\n",
              " 'best',\n",
              " 'and',\n",
              " 'most',\n",
              " 'beautiful',\n",
              " 'things',\n",
              " 'in',\n",
              " 'the',\n",
              " 'world',\n",
              " 'can',\n",
              " 'not',\n",
              " 'be',\n",
              " 'seen',\n",
              " 'or',\n",
              " 'even',\n",
              " 'touched',\n",
              " ',',\n",
              " 'they',\n",
              " 'must',\n",
              " 'be',\n",
              " 'felt',\n",
              " 'with',\n",
              " 'the',\n",
              " 'heart']"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Bigram\n",
        "quotes_bigrams = list(nltk.bigrams(quotes_tokens))\n",
        "print(quotes_bigrams)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GGjsA57qLFFo",
        "outputId": "4fe07ca9-4c1d-48f9-d78f-8459e7781742"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('The', 'best'), ('best', 'and'), ('and', 'most'), ('most', 'beautiful'), ('beautiful', 'things'), ('things', 'in'), ('in', 'the'), ('the', 'world'), ('world', 'can'), ('can', 'not'), ('not', 'be'), ('be', 'seen'), ('seen', 'or'), ('or', 'even'), ('even', 'touched'), ('touched', ','), (',', 'they'), ('they', 'must'), ('must', 'be'), ('be', 'felt'), ('felt', 'with'), ('with', 'the'), ('the', 'heart')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Trigram\n",
        "quotes_trigrams = list(nltk.trigrams(quotes_tokens))\n",
        "print(quotes_trigrams)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i5Tnw4uvmsWz",
        "outputId": "5a7d4162-ac66-4d63-bbb7-82a4a225ba11"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('The', 'best', 'and'), ('best', 'and', 'most'), ('and', 'most', 'beautiful'), ('most', 'beautiful', 'things'), ('beautiful', 'things', 'in'), ('things', 'in', 'the'), ('in', 'the', 'world'), ('the', 'world', 'can'), ('world', 'can', 'not'), ('can', 'not', 'be'), ('not', 'be', 'seen'), ('be', 'seen', 'or'), ('seen', 'or', 'even'), ('or', 'even', 'touched'), ('even', 'touched', ','), ('touched', ',', 'they'), (',', 'they', 'must'), ('they', 'must', 'be'), ('must', 'be', 'felt'), ('be', 'felt', 'with'), ('felt', 'with', 'the'), ('with', 'the', 'heart')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Ngrams\n",
        "\n",
        "```\n",
        "quotes_ngrams = list(nltk.ngrams(quotes_tokens, N))\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "a8Cv4g3mobYb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "quotes_ngrams = list(nltk.ngrams(quotes_tokens, 5))\n",
        "quotes_ngrams"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cv6ToyatodEE",
        "outputId": "c49b6d3f-7d8f-4fc3-a0ed-7120bb86db3f"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('The', 'best', 'and', 'most', 'beautiful'),\n",
              " ('best', 'and', 'most', 'beautiful', 'things'),\n",
              " ('and', 'most', 'beautiful', 'things', 'in'),\n",
              " ('most', 'beautiful', 'things', 'in', 'the'),\n",
              " ('beautiful', 'things', 'in', 'the', 'world'),\n",
              " ('things', 'in', 'the', 'world', 'can'),\n",
              " ('in', 'the', 'world', 'can', 'not'),\n",
              " ('the', 'world', 'can', 'not', 'be'),\n",
              " ('world', 'can', 'not', 'be', 'seen'),\n",
              " ('can', 'not', 'be', 'seen', 'or'),\n",
              " ('not', 'be', 'seen', 'or', 'even'),\n",
              " ('be', 'seen', 'or', 'even', 'touched'),\n",
              " ('seen', 'or', 'even', 'touched', ','),\n",
              " ('or', 'even', 'touched', ',', 'they'),\n",
              " ('even', 'touched', ',', 'they', 'must'),\n",
              " ('touched', ',', 'they', 'must', 'be'),\n",
              " (',', 'they', 'must', 'be', 'felt'),\n",
              " ('they', 'must', 'be', 'felt', 'with'),\n",
              " ('must', 'be', 'felt', 'with', 'the'),\n",
              " ('be', 'felt', 'with', 'the', 'heart')]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Stemming**\n",
        "Normalize words into its base form or root form\n",
        "\n",
        "PorterStemmer is a stemming algorithm implemented in the NLTK library.\n",
        "Reduces words to its base or root form."
      ],
      "metadata": {
        "id": "FGWfQl4NouqI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "pst = PorterStemmer()"
      ],
      "metadata": {
        "id": "qPdtAMnbpDEC"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pst.stem(\"having\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "0eRVDnPApzUh",
        "outputId": "49bf24ad-b978-41ab-9020-24994f397499"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'have'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words_to_stem = ['giving', 'give', 'given', 'gave']\n",
        "for words in words_to_stem:\n",
        "  print(words, \":\", pst.stem(words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JeSWIpuMp5XA",
        "outputId": "9dfc1219-6216-4431-c505-90bf305a5fa8"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "giving : give\n",
            "give : give\n",
            "given : given\n",
            "gave : gave\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lancaster Stemmer**\n",
        "\n",
        "More aggressive than the Porter Stemmer. Tends to be more liberal in its stemming and may produce shorter stems."
      ],
      "metadata": {
        "id": "rT2RkO81qSSF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import LancasterStemmer\n",
        "lanstem = LancasterStemmer()"
      ],
      "metadata": {
        "id": "3RNm_PZVqeQj"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words_to_stem = ['giving', 'give', 'given', 'gave']\n",
        "for words in words_to_stem:\n",
        "  print(words,\":\",lanstem.stem(words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XqiLpXL_qqBo",
        "outputId": "97d32419-417f-4292-d8db-a3f4f3c8e3f0"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "giving : giv\n",
            "give : giv\n",
            "given : giv\n",
            "gave : gav\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words_to_stem = ['bringing', 'brought', 'bring', 'brinjal']\n",
        "for words in words_to_stem:\n",
        "  print(words, \":\", lanstem.stem(words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Qn-GiOYq-sk",
        "outputId": "caa12c76-18e4-4a75-b0fa-d506859d54c9"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bringing : bring\n",
            "brought : brought\n",
            "bring : bring\n",
            "brinjal : brind\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Snowbell Stemmer**\n",
        "\n",
        "This framework supports multiple languages. The language used must be specified."
      ],
      "metadata": {
        "id": "FZhm45TwsFXE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "sbst = SnowballStemmer('english')"
      ],
      "metadata": {
        "id": "cV_hy2MWsSaA"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sbst.stem(\"Generation\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "7SPWKP6pseqS",
        "outputId": "60e1ea28-4f6b-4b42-9e15-e51d79498c83"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'generat'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words_to_stem = ['giving', 'gave','bringing', 'brough','having', 'have']\n",
        "for words in words_to_stem:\n",
        "  print(words, \":\", sbst.stem(words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0e_XZEYLsi37",
        "outputId": "6417c400-adf7-4842-b65e-593b7046a99e"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "giving : give\n",
            "gave : gave\n",
            "bringing : bring\n",
            "brough : brough\n",
            "having : have\n",
            "have : have\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Lemmatization**\n",
        "- Groups together different inflected forms of a word, called Lemma.\n",
        "- Somehow similar to Stemming, as it maps several words into one common root.\n",
        "- Output of Lemmatisation is a proper word."
      ],
      "metadata": {
        "id": "bhkEHeJCtQyl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "phRZGyM2tob9"
      },
      "execution_count": 1,
      "outputs": []
    }
  ]
}