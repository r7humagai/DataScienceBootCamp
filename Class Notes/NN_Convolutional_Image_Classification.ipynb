{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DR-eO17geWu"
      },
      "source": [
        "----\n",
        "#**Cat vs. Dog Image Classification using Convolutional Neural Networks**\n",
        "\n",
        "*This tutorial demonstrates the creation of a Convolutional Neural Network (CNN) for classifying images of cats and dogs. It covers data preprocessing, the construction of the CNN architecture, and the training of the model. By the end, you'll have a powerful image classification tool capable of distinguishing between these two common pet species with accuracy.*\n",
        "\n",
        "**Technlogies:** *TensorFlow, Keras.*\n",
        "\n",
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMefrVPCg-60"
      },
      "source": [
        "### Importing the libraries\n",
        "*Import the required libraries, TensorFlow and Keras's ImageDataGenerator for image preprocessing.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sCV30xyVhFbE"
      },
      "source": [
        "import tensorflow as tf\n",
        "from keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIleuCAjoFD8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "62002ba6-cbea-4dcd-e5ca-2894a5a115fb"
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.13.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ddy2wNMjZY-",
        "outputId": "bf49e679-be00-4f8e-a17b-463f31d3b6ac"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxQxCBWyoGPE"
      },
      "source": [
        "## Part 1 - Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvE-heJNo3GG"
      },
      "source": [
        "### Preprocessing the Training set\n",
        "\n",
        "1.   *Create an image data generator (train_datagen) with data augmentation settings, such as rescaling pixel values, shearing, zooming, and horizontal flipping.*\n",
        "\n",
        "2. *Load the training dataset using flow_from_directory. It specifies the target image size, batch size, and that it's a binary classification problem.*\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0koUcJMJpEBD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a700ecf0-ad65-4eca-9b35-9b2fd0311a73"
      },
      "source": [
        "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
        "                                   shear_range = 0.2,\n",
        "                                   zoom_range = 0.2,\n",
        "                                   horizontal_flip = True)\n",
        "\n",
        "training_set = train_datagen.flow_from_directory('/content/gdrive/MyDrive/Colab Notebooks/dataset/training_set',\n",
        "                                                 target_size = (64, 64),\n",
        "                                                 batch_size = 32,\n",
        "                                                 class_mode = 'binary')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 5009 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrCMmGw9pHys"
      },
      "source": [
        "### Preprocessing the Test set\n",
        "\n",
        "1. *Create a separate image data generator (test_datagen) for the test dataset, with rescaling and the same image size settings as the training set.*\n",
        "\n",
        "2. *Load the test dataset using flow_from_directory with similar settings as the training dataset.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SH4WzfOhpKc3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2da913a8-e5fa-46fe-9418-ade4ce941c1f"
      },
      "source": [
        "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "\n",
        "test_set = test_datagen.flow_from_directory('/content/gdrive/MyDrive/Colab Notebooks/dataset/test_set',\n",
        "                                            target_size = (64, 64),\n",
        "                                            batch_size = 32,\n",
        "                                            class_mode = 'binary')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 5000 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af8O4l90gk7B"
      },
      "source": [
        "## Part 2 - Building the CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ces1gXY2lmoX"
      },
      "source": [
        "### Initialising the CNN\n",
        "*Initialize a Sequential model named cnn for building the Convolutional Neural Network (CNN).*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SAUt4UMPlhLS"
      },
      "source": [
        "cnn = tf.keras.models.Sequential()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5YJj_XMl5LF"
      },
      "source": [
        "### Step 1 - Convolution\n",
        "*Add the first convolutional layer with 32 filters (feature maps), a 3x3 kernel, ReLU activation function, and specify the input shape for the first layer.*\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPzPrMckl-hV"
      },
      "source": [
        "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=[64, 64, 3]))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tf87FpvxmNOJ"
      },
      "source": [
        "### Step 2 - Pooling\n",
        "\n",
        "*Add a max-pooling layer to downsample the feature maps by a factor of 2. The pool_size parameter defines the size of the pooling window, and strides determines the step size.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncpqPl69mOac"
      },
      "source": [
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xaTOgD8rm4mU"
      },
      "source": [
        "### Adding a second convolutional layer\n",
        "*Add a second convolutional layer with the same settings as the first, followed by another max-pooling layer.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_-FZjn_m8gk"
      },
      "source": [
        "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu'))\n",
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmiEuvTunKfk"
      },
      "source": [
        "### Step 3 - Flattening\n",
        "\n",
        "*Flatten the feature maps into a 1D vector to prepare for the fully connected layers.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6AZeOGCvnNZn"
      },
      "source": [
        "cnn.add(tf.keras.layers.Flatten())"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAoSECOm203v"
      },
      "source": [
        "### Step 4 - Full Connection\n",
        "\n",
        "*Add a fully connected layer with 128 units and ReLU activation.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8GtmUlLd26Nq"
      },
      "source": [
        "cnn.add(tf.keras.layers.Dense(units=128, activation='relu'))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTldFvbX28Na"
      },
      "source": [
        "### Step 5 - Output Layer\n",
        "\n",
        "*The output layer with sigmoid activation is included for binary classification.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1p_Zj1Mc3Ko_"
      },
      "source": [
        "cnn.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6XkI90snSDl"
      },
      "source": [
        "## Part 3 - Training the CNN\n",
        "*The CNN is compiled with the 'adam' optimizer and 'binary_crossentropy' loss function. It is trained on the training set and evaluated on the test set over 25 epochs.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfrFQACEnc6i"
      },
      "source": [
        "### Compiling the CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NALksrNQpUlJ"
      },
      "source": [
        "cnn.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehS-v3MIpX2h"
      },
      "source": [
        "### Training the CNN on the Training set and evaluating it on the Test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUj1W4PJptta",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3eba0365-fc88-49a8-8e6f-21b1e5d79c4b"
      },
      "source": [
        "cnn.fit(x = training_set, validation_data = test_set, epochs = 25)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "157/157 [==============================] - 2830s 18s/step - loss: 0.5011 - accuracy: 0.8002 - val_loss: 1.6602 - val_accuracy: 0.2000\n",
            "Epoch 2/25\n",
            "157/157 [==============================] - 86s 547ms/step - loss: 0.4636 - accuracy: 0.8006 - val_loss: 0.8216 - val_accuracy: 0.3572\n",
            "Epoch 3/25\n",
            "157/157 [==============================] - 64s 410ms/step - loss: 0.4400 - accuracy: 0.8054 - val_loss: 1.2094 - val_accuracy: 0.2788\n",
            "Epoch 4/25\n",
            "157/157 [==============================] - 63s 403ms/step - loss: 0.4231 - accuracy: 0.8123 - val_loss: 1.1757 - val_accuracy: 0.3234\n",
            "Epoch 5/25\n",
            "157/157 [==============================] - 85s 543ms/step - loss: 0.4037 - accuracy: 0.8219 - val_loss: 0.7557 - val_accuracy: 0.5564\n",
            "Epoch 6/25\n",
            "157/157 [==============================] - 85s 542ms/step - loss: 0.3980 - accuracy: 0.8191 - val_loss: 1.0797 - val_accuracy: 0.3586\n",
            "Epoch 7/25\n",
            "157/157 [==============================] - 63s 401ms/step - loss: 0.3754 - accuracy: 0.8351 - val_loss: 1.1789 - val_accuracy: 0.4204\n",
            "Epoch 8/25\n",
            "157/157 [==============================] - 63s 402ms/step - loss: 0.3608 - accuracy: 0.8399 - val_loss: 1.1792 - val_accuracy: 0.4668\n",
            "Epoch 9/25\n",
            "157/157 [==============================] - 64s 407ms/step - loss: 0.3538 - accuracy: 0.8451 - val_loss: 1.1297 - val_accuracy: 0.4250\n",
            "Epoch 10/25\n",
            "157/157 [==============================] - 64s 409ms/step - loss: 0.3566 - accuracy: 0.8437 - val_loss: 1.0329 - val_accuracy: 0.5086\n",
            "Epoch 11/25\n",
            "157/157 [==============================] - 64s 406ms/step - loss: 0.3370 - accuracy: 0.8551 - val_loss: 1.2033 - val_accuracy: 0.4508\n",
            "Epoch 12/25\n",
            "157/157 [==============================] - 63s 401ms/step - loss: 0.3332 - accuracy: 0.8537 - val_loss: 1.2927 - val_accuracy: 0.4764\n",
            "Epoch 13/25\n",
            "157/157 [==============================] - 63s 401ms/step - loss: 0.3141 - accuracy: 0.8620 - val_loss: 0.9295 - val_accuracy: 0.5352\n",
            "Epoch 14/25\n",
            "157/157 [==============================] - 84s 533ms/step - loss: 0.3220 - accuracy: 0.8603 - val_loss: 0.9643 - val_accuracy: 0.5520\n",
            "Epoch 15/25\n",
            "157/157 [==============================] - 63s 399ms/step - loss: 0.3049 - accuracy: 0.8700 - val_loss: 1.2671 - val_accuracy: 0.4972\n",
            "Epoch 16/25\n",
            "157/157 [==============================] - 62s 395ms/step - loss: 0.3054 - accuracy: 0.8710 - val_loss: 0.8559 - val_accuracy: 0.6152\n",
            "Epoch 17/25\n",
            "157/157 [==============================] - 84s 535ms/step - loss: 0.2917 - accuracy: 0.8732 - val_loss: 1.1138 - val_accuracy: 0.5306\n",
            "Epoch 18/25\n",
            "157/157 [==============================] - 62s 398ms/step - loss: 0.2793 - accuracy: 0.8800 - val_loss: 0.9735 - val_accuracy: 0.5932\n",
            "Epoch 19/25\n",
            "157/157 [==============================] - 63s 404ms/step - loss: 0.2708 - accuracy: 0.8868 - val_loss: 1.1963 - val_accuracy: 0.5032\n",
            "Epoch 20/25\n",
            "157/157 [==============================] - 62s 397ms/step - loss: 0.2658 - accuracy: 0.8854 - val_loss: 1.3899 - val_accuracy: 0.5050\n",
            "Epoch 21/25\n",
            "157/157 [==============================] - 62s 397ms/step - loss: 0.2647 - accuracy: 0.8864 - val_loss: 1.0129 - val_accuracy: 0.6082\n",
            "Epoch 22/25\n",
            "157/157 [==============================] - 62s 394ms/step - loss: 0.2593 - accuracy: 0.8872 - val_loss: 1.2702 - val_accuracy: 0.5292\n",
            "Epoch 23/25\n",
            "157/157 [==============================] - 63s 404ms/step - loss: 0.2399 - accuracy: 0.8982 - val_loss: 1.4111 - val_accuracy: 0.5238\n",
            "Epoch 24/25\n",
            "157/157 [==============================] - 62s 397ms/step - loss: 0.2367 - accuracy: 0.9046 - val_loss: 1.2027 - val_accuracy: 0.6008\n",
            "Epoch 25/25\n",
            "157/157 [==============================] - 61s 390ms/step - loss: 0.2222 - accuracy: 0.9080 - val_loss: 1.1483 - val_accuracy: 0.5994\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7a594e73bc70>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3PZasO0006Z"
      },
      "source": [
        "## Part 4 - Making a single prediction\n",
        "\n",
        "*The CNN is used to predict whether an image (in this case, 'cat_or_dog_1.jpg') is a cat or a dog. The image is loaded, processed, and a prediction is made based on the model's output. The result is displayed as 'cat' or 'dog' based on the predicted class.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsSiWEJY1BPB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9605726c-9c9a-453f-b391-3aec64118c1e"
      },
      "source": [
        "import numpy as np\n",
        "from keras.preprocessing import image\n",
        "test_image = image.load_img('/content/gdrive/MyDrive/Colab Notebooks/dataset/single_prediction/cat_or_dog_1.jpg', target_size = (64, 64))\n",
        "test_image = image.img_to_array(test_image)\n",
        "test_image = np.expand_dims(test_image, axis = 0)\n",
        "result = cnn.predict(test_image)\n",
        "training_set.class_indices\n",
        "if result[0][0] == 1:\n",
        "  prediction = 'dog'\n",
        "else:\n",
        "  prediction = 'cat'\n",
        "\n",
        "print(prediction)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 25ms/step\n",
            "dog\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "----\n",
        "**Conclusion**\n",
        "\n",
        "\n",
        "In conclusion, this tutorial provides a comprehensive walkthrough of Convolutional Neural Network (CNN) development for precise cat and dog image classification. By following these detailed steps, users gain valuable insights into data preprocessing, CNN architecture, and model training, equipping them with the knowledge necessary to construct robust image classification systems.\n",
        "\n",
        "----"
      ],
      "metadata": {
        "id": "F-X_wA49eJMV"
      }
    }
  ]
}